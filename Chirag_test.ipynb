{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78175c2a-40df-4fec-af30-6d6a5ef90396",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5028ce-8e8d-4b7c-aab7-31c2273de590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import math\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('.\\Data\\\\text\\Subtask_1_train.json', encoding=\"utf-8\")\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc49206d-a4d4-486a-907c-ae1b6fa24f92",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_even_datapoints(datapoints, n):\n",
    "    random.seed(42)\n",
    "    dp_by_label = defaultdict(list)\n",
    "    for dp in tqdm(datapoints, desc='Reading Datapoints'):\n",
    "        dp_by_label[dp['label']].append(dp)\n",
    "\n",
    "    unique_labels = [0, 1, 2]\n",
    "\n",
    "    split = n//len(unique_labels)\n",
    "\n",
    "    result_datapoints = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        result_datapoints.extend(random.sample(dp_by_label[label], split))\n",
    "\n",
    "    return result_datapoints\n",
    "\n",
    "train_dataset = get_even_datapoints(data['train'], train)\n",
    "validation_dataset = get_even_datapoints(data['validation'], validation)\n",
    "test_dataset = get_even_datapoints(data['test'], test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c8f50c-7a7a-4ecf-998c-90fad99f2da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for conversations, emotion-cause pairs, and emotion labels\n",
    "x_data = []\n",
    "y_cause_labels = []\n",
    "y_emotion_labels = []\n",
    "\n",
    "# Iterate through each conversation in the dataset\n",
    "for conv in data:\n",
    "    # Extract conversation, emotion-cause pairs, and emotion labels\n",
    "    conversation = conv['conversation']\n",
    "    emotion_cause_pairs = conv['emotion-cause_pairs']\n",
    "    \n",
    "    # Extract emotion labels from each utterance in the conversation\n",
    "    emotion_labels = [utterance['emotion'] for utterance in conversation]\n",
    "\n",
    "    # Append to the respective lists\n",
    "    x_data.append(conversation)\n",
    "    y_cause_labels.extend(emotion_cause_pairs)\n",
    "    y_emotion_labels.append(emotion_labels) # figure out if its append or extend\n",
    "\n",
    "# Print the extracted data\n",
    "# print(\"Conversations:\")\n",
    "# for conv in x_data:\n",
    "#     print(conv)\n",
    "\n",
    "# print(\"\\nEmotion-Cause Pairs:\")\n",
    "# for pair in y_cause_labels:\n",
    "#     print(pair)\n",
    "\n",
    "# print(\"\\nEmotion Labels:\")\n",
    "# print(y_emotion_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae647e6d-3c05-4150-beb1-563a287edfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_emotion_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d8d90-531c-49b2-8d39-5e0276bd0341",
   "metadata": {},
   "source": [
    "Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a13f7ba-57a7-46dd-a0de-12e59297ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_conv, dev_conv = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "#todo: with above x_data and y_label get x_train, y_ label, x_dev and y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4266ffb7-699d-4ef8-be59-9e28d335eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1099\n",
      "Development set size: 275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and development sets\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(x_data, y_emotion_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the sizes of the sets\n",
    "print(\"Training set size:\", len(x_train))\n",
    "print(\"Development set size:\", len(x_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0ecda2-e2be-4983-8ccd-09f7d79df4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(sequences: List[str], batch_size: int) -> List[List[str]]:\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "\n",
    "    batch_list=[]\n",
    "\n",
    "    last_index=len(sequences)-1\n",
    "    \n",
    "    for index in range(math.ceil(len(sequences)/batch_size)):\n",
    "        \n",
    "        if index+batch_size:\n",
    "            batch_list.append(sequences[index:index+batch_size])\n",
    "        else:\n",
    "            batch_list.append(sequences[index:last_index])\n",
    "    # DONE\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0671ab44-0bb1-46f4-854c-321e4af73ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass BatchTokenizer:\\n    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\\n\\n    def __init__(self, model_name=\"bert-base-uncased\"):\\n        \"\"\"Initializes the tokenizer\\n\\n        Args:\\n            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\\n        \"\"\"\\n        self.conv_tokenizer = AutoTokenizer.from_pretrained(model_name)\\n        self.model_name = model_name\\n    \\n    def get_sep_token(self,):\\n        return self.conv_tokenizer.sep_token\\n    \\n    def __call__(self, conv_batch: List[dict]) -> List[dict[str]]:\\n        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\\n\\n        We return a dictionary of tensors per the huggingface model specification.\\n\\n        Args:\\n            batch (List[str]): A List of sentence strings\\n\\n        Returns:\\n            Dict: The dictionary of token specifications provided by HuggingFace\\n        \"\"\"\\n        # The HF tokenizer will PAD for us, and additionally combine \\n        # The two sentences deimited by the [SEP] token.\\n        # combined_texts = [f\"{utterance[\\'speaker\\']}: {utterance[\\'text\\']}\" for utterance in conv_batch[\"conversation\"]]\\n        # print(combined_texts)\\n        \\n        # print(conv_batch)\\n        combined_single_texts=[]\\n        batch_texts=[]\\n        for conv in conv_batch:\\n            #print(conv)\\n            combined_single_texts=[]\\n            for utterance in conv:\\n                combined_single_texts.append(f\"{utterance[\\'speaker\\']}: {utterance[\\'text\\']}\")\\n            batch_texts.append(combined_single_texts)\\n        #print(batch_texts)\\n        encoded=[]\\n        for batch in batch_texts:\\n            enc = self.conv_tokenizer(\\n                batch,\\n                padding=True,\\n                return_token_type_ids=False,\\n                return_tensors=\\'pt\\'\\n            )\\n            encoded.append(enc)\\n        # print(encoded)\\n        return enc\\n    \\n\\n# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\\ntokenizer = BatchTokenizer()\\nx = tokenizer(x_train[0:10])\\nprint(x)\\n#tokenizer.conv_tokenizer.batch_decode(x[\"input_ids\"])\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this for now\n",
    "'''\n",
    "class BatchTokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
    "        \"\"\"\n",
    "        self.conv_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def get_sep_token(self,):\n",
    "        return self.conv_tokenizer.sep_token\n",
    "    \n",
    "    def __call__(self, conv_batch: List[dict]) -> List[dict[str]]:\n",
    "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
    "\n",
    "        We return a dictionary of tensors per the huggingface model specification.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            Dict: The dictionary of token specifications provided by HuggingFace\n",
    "        \"\"\"\n",
    "        # The HF tokenizer will PAD for us, and additionally combine \n",
    "        # The two sentences deimited by the [SEP] token.\n",
    "        # combined_texts = [f\"{utterance['speaker']}: {utterance['text']}\" for utterance in conv_batch[\"conversation\"]]\n",
    "        # print(combined_texts)\n",
    "        \n",
    "        # print(conv_batch)\n",
    "        combined_single_texts=[]\n",
    "        batch_texts=[]\n",
    "        for conv in conv_batch:\n",
    "            #print(conv)\n",
    "            combined_single_texts=[]\n",
    "            for utterance in conv:\n",
    "                combined_single_texts.append(f\"{utterance['speaker']}: {utterance['text']}\")\n",
    "            batch_texts.append(combined_single_texts)\n",
    "        #print(batch_texts)\n",
    "        encoded=[]\n",
    "        for batch in batch_texts:\n",
    "            enc = self.conv_tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                return_token_type_ids=False,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded.append(enc)\n",
    "        # print(encoded)\n",
    "        return enc\n",
    "    \n",
    "\n",
    "# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\n",
    "tokenizer = BatchTokenizer()\n",
    "x = tokenizer(x_train[0:10])\n",
    "print(x)\n",
    "#tokenizer.conv_tokenizer.batch_decode(x[\"input_ids\"])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f01a3a17-0177-444f-959c-30f10b14bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d81fa9-4849-4527-8d41-7b1a6b85fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class BatchTokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        \"\"\"Initializes the tokenizer.\n",
    "\n",
    "        Args:\n",
    "            model_name (str, optional): Pretrained model name. Defaults to \"bert-base-uncased\".\n",
    "        \"\"\"\n",
    "        self.conv_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def get_sep_token(self):\n",
    "        return self.conv_tokenizer.sep_token\n",
    "    \n",
    "    def __call__(self, conv_batch: List[Dict[str, List[Dict[str, str]]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Uses the Hugging Face tokenizer to tokenize and pad a batch.\n",
    "\n",
    "        Args:\n",
    "            conv_batch (List[Dict[str, List[Dict[str, str]]]]): A list of conversations.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, torch.Tensor]: The dictionary of token specifications provided by Hugging Face.\n",
    "        \"\"\"\n",
    "        # The HF tokenizer will PAD for us, and additionally combine \n",
    "        # the sentences delimited by the [SEP] token.\n",
    "        \n",
    "        combined_texts = []\n",
    "        for conv in conv_batch:\n",
    "            # Check if the conversation is in the expected format\n",
    "            if isinstance(conv, dict) and \"conversation\" in conv:\n",
    "                for utterance in conv[\"conversation\"]:\n",
    "                    # Check if the utterance has the expected keys\n",
    "                    if \"speaker\" in utterance and \"text\" in utterance:\n",
    "                        combined_texts.append(f\"{utterance['speaker']}: {utterance['text']}\")\n",
    "                    else:\n",
    "                        print(\"Utterance is missing 'speaker' or 'text' keys:\", utterance)\n",
    "            else:\n",
    "                print(\"Conversation is not a dictionary or missing 'conversation' key:\", conv)\n",
    "\n",
    "        if not combined_texts:\n",
    "            raise ValueError(\"No texts found for tokenization\")\n",
    "\n",
    "\n",
    "        encodings = self.conv_tokenizer(\n",
    "            combined_texts,\n",
    "            padding=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return encodings\n",
    "\n",
    "# Example of how to use the batch tokenizer\n",
    "tokenizer = BatchTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96d2f14-34ee-4c01-8b34-91a77b9015bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  9558,  1024,  3100,  1010,  2156,  2008,  6397,  3124,  2157,\n",
      "          2045,  1029,  1045,  2572,  6069, 24234,  2010,  2132,  1999,  2101,\n",
      "          1012,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  9558,  1024,  2821, 26114,  1010,  2026,  2502,  3496,  2003,\n",
      "          2746,  2039,  1012,  2502,  3496,  2746,  2039,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 13814,  1024,  2065,  2017,  2056,  1010,  1000,  2502, 12967,\n",
      "         14068,  1010, 25054,  2039,  1012,  1000,  2052,  2016,  3305,  1996,\n",
      "          4489,  1029,   102,     0,     0,     0,     0],\n",
      "        [  101,  9018,  1024, 10958,  2818,  1029,  2054,  2024,  2017,  2725,\n",
      "          1029,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5586,  1024,  2821,  2879,  1010,  1045,  2074,  2064,  2025,\n",
      "          3422,  1012,  2009,  2003,  2205, 12459,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9018,  1024,  2009,  2003,  1037, 22939,  4842,  3293,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5586,  1024,  2821,  3398,  2092,  1010,  2017,  2113,  2033,\n",
      "          1010, 10834,  1010, 10198,  1010,  6289, 23644,   999,   999,   999,\n",
      "           102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 27555,  1024, 10733,  6959,   999,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5811,  1024,  1045,  2097,  2131,  2009,   999,  1045,  2097,\n",
      "          2131,  2008,   999,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 27555,  1024,  7632,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5811,  1024,  7632,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9018,  1024, 27593,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  2428,  1029,   999,  2092,  1010,  2009,  2003,\n",
      "          2074,  2066,  3071,  2842,  4545,  1012,  2009,  2003,  2288,  4734,\n",
      "          1010,  3681,  1010,  1998, 21098,  1012,   102],\n",
      "        [  101,  2957,  1005,  1055,  3058,  1024,  2092,  1010,  1045,  2074,\n",
      "          2359,  2000,  2156,  2073,  2017,  2973,  1012,  2085,  1010,  2507,\n",
      "          2033,  1996,  2778,  1012,   102,     0,     0],\n",
      "        [  101,  9018,  1024,  2821,  2026,  2643,   999,  2821,  2026,  2643,\n",
      "           999,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  6289,  2092,  1010,  2023,  2003,  1996,  2542,\n",
      "          2282,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  2035,  2157,  1012,  2023,  2003,  1996,  3829,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  1996,  5010,  1012,  2092,  2009,  2003,  3492,\n",
      "          2172,  2115,  5171,  1012,  1012,  1012,  5010,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1005,  1055,  3058,  1024,  2057,  2024,  2145,  2006,\n",
      "          2023,  2217,  1997,  1996,  2341,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1005,  1055,  3058,  1024,  3398,  1010,  2021,  1045,\n",
      "          2106,  2025,  2131,  2000,  2156,  2009,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  2821,  5607,   999,  2672,  2279,  2051,  1012,\n",
      "          4283,  2005,  1037,  8403,  3944,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9018,  1024,  2061,  8529,  1010,  2040,  2001,  2016,  1029,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2957,  1024,  2821,  1010,  2008,  2001,  1996,  6397,  3058,\n",
      "          2008,  1045,  2409,  2017,  2055,  1010,  2016,  2170,  1998,  7237,\n",
      "          2009,  2000,  2651,  1012,   102,     0,     0],\n",
      "        [  101,  9018,  1024,  2106,  2017,  2066,  2014,  1029,  1998,  1045,\n",
      "          2572,  2074,  4851,  2004,  1037,  2767,  1010,  2138,  1045,  2572,\n",
      "          6135,  2986,  2007,  2023,  1012,   102,     0],\n",
      "        [  101,  2957,  1024,  2092,  1010,  2017,  4025,  2986,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]])}\n",
      "['[CLS] joey : okay, see that blind guy right there? i am gonna bash his head in later. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] joey : oh umm, my big scene is coming up. big scene coming up. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] chandler : if you said, \" big lima bean, bubbling up. \" would she understand the difference? [SEP] [PAD] [PAD] [PAD] [PAD]', '[CLS] monica : rach? what are you doing? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] rachel : oh boy, i just can not watch. it is too scary! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] monica : it is a diaper commercial. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] rachel : oh yeah well, you know me, babies, responsibilities, ahhh!!! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] caitlin : pizza delivery! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] ross : i will get it! i will get that! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] caitlin : hi! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] ross : hi! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] monica : ow! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] richard : really?! well, it is just like everyone else apartment. it is got rooms, walls, and ceilings. [SEP]', \"[CLS] richard's date : well, i just wanted to see where you lived. now, give me the tour. [SEP] [PAD] [PAD]\", '[CLS] monica : oh my god! oh my god! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] richard : ah well, this is the living room. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] richard : all right. this is the kitchen. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] richard : the bedroom. well it is pretty much your typical... bedroom. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', \"[CLS] richard's date : we are still on this side of the door. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\", \"[CLS] richard's date : yeah, but i did not get to see it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\", '[CLS] richard : oh shoot! maybe next time. thanks for a lovely evening. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] monica : so um, who was she? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] richard : oh, that was the blind date that i told you about, she called and switched it to today. [SEP] [PAD] [PAD]', '[CLS] monica : did you like her? and i am just asking as a friend, because i am totally fine with this. [SEP] [PAD]', '[CLS] richard : well, you seem fine. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "batch_encodings = tokenizer([train_conv[0], train_conv[1]])  # Pass a batch of conversations\n",
    "print(batch_encodings)\n",
    "decoded_texts = tokenizer.conv_tokenizer.batch_decode(batch_encodings[\"input_ids\"])\n",
    "print(decoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae439a9-f580-47b8-ba18-a0a1b0b2fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef chunk(lst, n):\\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\\n    for i in range(0, len(lst), n):\\n        yield lst[i:i + n]\\n\\nbatch_size = 16\\n        \\n# Notice that since we use huggingface, we tokenize and\\n# encode in all at once!\\ntokenizer = BatchTokenizer()\\n\\ntrain_input_batches = [b for b in chunk(x_train, batch_size)]\\n# Tokenize + encode\\ntrain_input_batches = [tokenizer(batch) for batch in train_input_batches]\\n\\ndev_input_batches = [b for b in chunk(x_dev, batch_size)]\\n\\n# Tokenize + encode\\ndev_input_batches = [tokenizer(batch) for batch in dev_input_batches]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "batch_size = 16\n",
    "        \n",
    "# Notice that since we use huggingface, we tokenize and\n",
    "# encode in all at once!\n",
    "tokenizer = BatchTokenizer()\n",
    "\n",
    "train_input_batches = [b for b in chunk(x_train, batch_size)]\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(batch) for batch in train_input_batches]\n",
    "\n",
    "dev_input_batches = [b for b in chunk(x_dev, batch_size)]\n",
    "\n",
    "# Tokenize + encode\n",
    "dev_input_batches = [tokenizer(batch) for batch in dev_input_batches]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e72b245-b376-4fca-9569-9552d12b36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 16\n",
      "Batch size: 11\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Okay , see that blind guy right there ? I am gonna bash his head in later .', 'speaker': 'Joey', 'emotion': 'joy'}, {'utterance_ID': 2, 'text': 'Oh umm , my big scene is coming up . Big scene coming up .', 'speaker': 'Joey', 'emotion': 'joy'}, {'utterance_ID': 3, 'text': 'If you said , \" Big lima bean , bubbling up . \" Would she understand the difference ?', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Rach ? What are you doing ?', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'Oh boy , I just can not watch . It is too scary !', 'speaker': 'Rachel', 'emotion': 'fear'}, {'utterance_ID': 6, 'text': 'It is a diaper commercial .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'Oh yeah well , you know me , babies , responsibilities , ahhh ! ! !', 'speaker': 'Rachel', 'emotion': 'fear'}, {'utterance_ID': 8, 'text': 'Pizza delivery !', 'speaker': 'Caitlin', 'emotion': 'joy'}, {'utterance_ID': 9, 'text': 'I will get it ! I will get that !', 'speaker': 'Ross', 'emotion': 'joy'}, {'utterance_ID': 10, 'text': 'Hi !', 'speaker': 'Caitlin', 'emotion': 'joy'}, {'utterance_ID': 11, 'text': 'Hi !', 'speaker': 'Ross', 'emotion': 'joy'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Ow !', 'speaker': 'Monica', 'emotion': 'anger'}, {'utterance_ID': 2, 'text': 'Really ? ! Well , it is just like everyone else apartment . It is got rooms , walls , and ceilings .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 3, 'text': 'Well , I just wanted to see where you lived . Now , give me the tour .', 'speaker': \"Richard's Date\", 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Oh my God ! Oh my God !', 'speaker': 'Monica', 'emotion': 'surprise'}, {'utterance_ID': 5, 'text': 'Ah well , this is the living room .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'All right . This is the kitchen .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'The bedroom . Well it is pretty much your typical ... bedroom .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'We are still on this side of the door .', 'speaker': \"Richard's Date\", 'emotion': 'neutral'}, {'utterance_ID': 9, 'text': 'Yeah , but I did not get to see it .', 'speaker': \"Richard's Date\", 'emotion': 'sadness'}, {'utterance_ID': 10, 'text': 'Oh shoot ! Maybe next time . Thanks for a lovely evening .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 11, 'text': 'So um , who was she ?', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'Oh , that was the blind date that I told you about , she called and switched it to today .', 'speaker': 'Richard', 'emotion': 'neutral'}, {'utterance_ID': 13, 'text': 'Did you like her ? And I am just asking as a friend , because I am totally fine with this .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 14, 'text': 'Well , you seem fine .', 'speaker': 'Richard', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'It is so secluded up here .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'I know . I like it up here .', 'speaker': 'Paul', 'emotion': 'joy'}, {'utterance_ID': 3, 'text': 'I feel like we are the only two people in the world . Oops . Sorry .', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 4, 'text': 'What the matter honey ? Did you see a little mouse ?', 'speaker': 'Paul', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'No ... no ! Big bear ! Big bear outside ! I think I ... I ... would you ... actually , would you go check on that ?', 'speaker': 'Rachel', 'emotion': 'fear'}, {'utterance_ID': 6, 'text': 'Honey , we do not have any bears here .', 'speaker': 'Paul', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'Well , okay . Would ... would you get me a Diet Coke ?', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'Okay . I will be right back .', 'speaker': 'Paul', 'emotion': 'neutral'}, {'utterance_ID': 9, 'text': 'Okay . What are you doing here ? !', 'speaker': 'Rachel', 'emotion': 'anger'}, {'utterance_ID': 10, 'text': 'What are you doing here ? !', 'speaker': 'Ross', 'emotion': 'surprise'}, {'utterance_ID': 11, 'text': 'I came with Paul !', 'speaker': 'Rachel', 'emotion': 'anger'}, {'utterance_ID': 12, 'text': 'Yeah , I recognize the ankles !', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 13, 'text': 'Get up !', 'speaker': 'Rachel', 'emotion': 'anger'}, {'utterance_ID': 14, 'text': 'Here you go honey !', 'speaker': 'Paul', 'emotion': 'joy'}, {'utterance_ID': 15, 'text': 'Ahh . Thank you !', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 16, 'text': 'Diet Coke', 'speaker': 'Paul', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'You know I gotta tell you , sometimes I just ... I do not get Chandler .', 'speaker': 'Joey', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'If you wanna give Joey a Christmas present that disrupts the entire building , why not get him something a little bit more subtle , like a wrecking ball , or a phial of small pox to release in the hallway ?', 'speaker': 'Chandler', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'I see . You know umm , Phoebe makes sock bunnies .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'No !', 'speaker': 'Monica', 'emotion': 'fear'}, {'utterance_ID': 3, 'text': 'No , she does not .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Uh Phoebe , what she makes ... that is uh ... they are sock rabbits .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'They are completely different ... Okay !', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'Okay !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 7, 'text': 'Okay !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 8, 'text': 'I did not make it !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 9, 'text': 'I am sorry !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 10, 'text': 'I totally forgot about tonight and the fact that we are supposed to make the presents !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 11, 'text': 'Oh , it is okay . I do not ... uh ...', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'No ... no , it is not okay !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 13, 'text': 'It is not !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 14, 'text': 'I mean you were just ... You are so incredible !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 15, 'text': 'You went through all this time and effort to make this tape for me !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 16, 'text': 'You know I am just gonna ... I , I am gonna make this up to you !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 17, 'text': 'I will !', 'speaker': 'Monica', 'emotion': 'sadness'}, {'utterance_ID': 18, 'text': 'I ... I am going to cook you anything you want in here , and I am going to do anything you want in there !', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 19, 'text': 'Well , I did put a lot of thought in the tape .', 'speaker': 'Chandler', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'It is not just the drum noise .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Every five minutes , Joey throws his sticks in the air , and I have to hear , \" Oh my eye !', 'speaker': 'Monica', 'emotion': 'anger'}, {'utterance_ID': 3, 'text': 'Oh god , my eye ! \"', 'speaker': 'Monica', 'emotion': 'anger'}, {'utterance_ID': 4, 'text': 'I mean , it is so annoying .', 'speaker': 'Monica', 'emotion': 'anger'}, {'utterance_ID': 5, 'text': 'Yes , thank you . You see , this is how normal people are supposed to react to drums .', 'speaker': 'Phoebe', 'emotion': 'anger'}, {'utterance_ID': 6, 'text': 'Phoebe , you got Joey drums to annoy Rachel , so she would not wanna live there anymore ?', 'speaker': 'Monica', 'emotion': 'surprise'}, {'utterance_ID': 7, 'text': 'Maybe on some level .', 'speaker': 'Phoebe', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'So your dad dropped by . He is a pleasant man !', 'speaker': 'Ross', 'emotion': 'anger'}, {'utterance_ID': 2, 'text': 'Oh no', 'speaker': 'Rachel', 'emotion': 'fear'}, {'utterance_ID': 3, 'text': 'I would better go .', 'speaker': 'Phoebe', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Ross I am so sorry . Okay . I ... I will promise I will straighten this out with him tomorrow in person , or via e ... mail .', 'speaker': 'Rachel', 'emotion': 'sadness'}, {'utterance_ID': 5, 'text': 'I do not care about your dad ! I care about Mona ! She was there and now she is totally freaked out !', 'speaker': 'Ross', 'emotion': 'anger'}, {'utterance_ID': 6, 'text': 'Oh okay , I will fix that to . What her e ... mail address ?', 'speaker': 'Rachel', 'emotion': 'sadness'}, {'utterance_ID': 7, 'text': 'Rachel !', 'speaker': 'Ross', 'emotion': 'anger'}, {'utterance_ID': 8, 'text': 'All right , I promise . I will fix this . I swear . I will ... I will ... I will ... I will talk to her .', 'speaker': 'Rachel', 'emotion': 'sadness'}, {'utterance_ID': 9, 'text': 'Okay !', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 10, 'text': 'Okay .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 11, 'text': 'Thank you !', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'That is it ? !', 'speaker': 'Phoebe', 'emotion': 'surprise'}, {'utterance_ID': 13, 'text': 'You call that a fight ?', 'speaker': 'Phoebe', 'emotion': 'surprise'}, {'utterance_ID': 14, 'text': 'Come on !', 'speaker': 'Phoebe', 'emotion': 'anger'}, {'utterance_ID': 15, 'text': '\" We were on a break ! \"', 'speaker': 'Phoebe', 'emotion': 'anger'}, {'utterance_ID': 16, 'text': '\" No we were not ! \"', 'speaker': 'Phoebe', 'emotion': 'anger'}, {'utterance_ID': 17, 'text': 'What happened to you two ?', 'speaker': 'Phoebe', 'emotion': 'surprise'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Okay , look , he is not gonna hurt them , right ?', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Do you always have to bring him here ?', 'speaker': 'Monica', 'emotion': 'anger'}, {'utterance_ID': 3, 'text': 'I did not wanna leave him alone .', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Alright ?', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'We ... we had our first fight this morning .', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'I think it has to do with my working late .', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'I said some things that I did not mean , and he ... he threw some faeces ...', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'You know , if you are gonna work late , I could look in on him for you .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 9, 'text': 'Oh , that would be great !', 'speaker': 'Ross', 'emotion': 'joy'}, {'utterance_ID': 10, 'text': 'Okay , but if you do , make sure it seems like you are there to see him , okay , and you are not like doing it as a favour to me .', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 11, 'text': 'Okay , but if he asks , I am not going to lie .', 'speaker': 'Chandler', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Uhh , Rachel , my parents', 'speaker': 'Joshua', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Ohh ! It is so nice to meet you . Hello .', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 3, 'text': 'Hello . Well , Joshua , that $ 500 was for groceries .', 'speaker': 'Mrs. Burgin', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'What ?', 'speaker': 'Rachel', 'emotion': 'surprise'}, {'utterance_ID': 5, 'text': 'This ... this , no , oh no , no ... no ... no , this is not ... that is ... that is not what it is .', 'speaker': 'Rachel', 'emotion': 'fear'}, {'utterance_ID': 6, 'text': 'See , see , okay , I work in fashion , see and ... and , this is a real dress actually .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'It is ... it is , they are ... they are wearing it in Milan , so part of my job is too wear the clothes , and then I see how people respond , and then I report back to my superiors at Bloomingdale’s .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'Maybe in L . A ?', 'speaker': 'Mrs. Burgin', 'emotion': 'neutral'}, {'utterance_ID': 9, 'text': 'Yes !', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 10, 'text': 'There you go .', 'speaker': 'Joshua', 'emotion': 'joy'}, {'utterance_ID': 11, 'text': 'So , have you kids eaten yet ?', 'speaker': 'Mr. Burgin', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'Well , we were going to do that after ... I mean umm , next .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 13, 'text': 'Well , we are starving , why do not we all go get something to eat ?', 'speaker': 'Mr. Burgin', 'emotion': 'neutral'}, {'utterance_ID': 14, 'text': 'Oh , yeah , well ... Yeah , no use wasting this baby , just lyin around the house .', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 15, 'text': 'So ... We go eat .', 'speaker': 'Mr. Burgin', 'emotion': 'neutral'}, {'utterance_ID': 16, 'text': 'Yes .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 17, 'text': 'You will wear that . We will be eating , and of course , you will be wearing that .', 'speaker': 'Mr. Burgin', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Philadelphia .', 'speaker': 'Earl', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Oh my God ! So was she ! Oh , I have got ... I have got goose bumps .', 'speaker': 'Phoebe', 'emotion': 'surprise'}, {'utterance_ID': 3, 'text': 'Really ?', 'speaker': 'Earl', 'emotion': 'surprise'}, {'utterance_ID': 4, 'text': 'Well , you know I am wearing layers and it is warm .', 'speaker': 'Phoebe', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'Yeah .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'But if ... no look , okay . These jerks might not care about you , but the universe does ! And that says a lot !', 'speaker': 'Phoebe', 'emotion': 'anger'}, {'utterance_ID': 7, 'text': 'Did you hear that ? !', 'speaker': 'Earl', 'emotion': 'surprise'}, {'utterance_ID': 8, 'text': 'I do not need you guys to care about me !', 'speaker': 'Earl', 'emotion': 'anger'}, {'utterance_ID': 9, 'text': 'Because the universe cares !', 'speaker': 'Earl', 'emotion': 'anger'}, {'utterance_ID': 10, 'text': 'The whole universe !', 'speaker': 'Earl', 'emotion': 'anger'}, {'utterance_ID': 11, 'text': 'I really wished they would care just a little bit though .', 'speaker': 'Earl', 'emotion': 'sadness'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Uh , like , could these margaritas be any stronger ?', 'speaker': 'Gerston', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Hey , Chandler .', 'speaker': 'Gerston', 'emotion': 'neutral'}, {'utterance_ID': 3, 'text': 'Hello , Mr . Bing .', 'speaker': 'Santos', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Loved your Stevie Wonder last night .', 'speaker': 'Petrie', 'emotion': 'joy'}, {'utterance_ID': 5, 'text': 'Thanks . Listen , about the weekly numbers , I am gonna need them on my desk by 9 : 00 .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'Sure .', 'speaker': 'Santos', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'No problem .', 'speaker': 'Gerston', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'You have to give em something , you know .', 'speaker': 'Chandler', 'emotion': 'joy'}, {'utterance_ID': 9, 'text': 'Okay , now that was Gerston , Santos , and who the guy with the moustache ?', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 10, 'text': 'Petrie .', 'speaker': 'Phoebe', 'emotion': 'neutral'}, {'utterance_ID': 11, 'text': 'Petrie , right , right . Okay , some people gonna be working', 'speaker': 'Chandler', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Oh my God ! Oh my God ! !', 'speaker': 'Rachel', 'emotion': 'surprise'}, {'utterance_ID': 2, 'text': 'Still crying ?', 'speaker': 'Monica', 'emotion': 'surprise'}, {'utterance_ID': 3, 'text': 'Like a little girl .', 'speaker': 'Rachel', 'emotion': 'disgust'}, {'utterance_ID': 4, 'text': 'You know , I only know of two surefire ways to shut a man up . And one of them is sex .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'What the other one ?', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'I do not know , I have never had to use the other one .', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 7, 'text': 'I am just saying you know , if we are having sex , he is not gonna be talking .', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 8, 'text': 'Oh that is right .', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 9, 'text': 'You are the talker .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 10, 'text': 'Anyway uh , great idea !', 'speaker': 'Rachel', 'emotion': 'joy'}, {'utterance_ID': 11, 'text': 'Umm , I gotta go to the store ; I told him that I would buy him some more tissues .', 'speaker': 'Rachel', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'Oh , we have some', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 13, 'text': 'No you do not !', 'speaker': 'Rachel', 'emotion': 'anger'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Hey sweetie !', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 2, 'text': 'Hey !', 'speaker': 'Chandler', 'emotion': 'joy'}, {'utterance_ID': 3, 'text': 'There is no back to this couch !', 'speaker': 'Chandler', 'emotion': 'surprise'}, {'utterance_ID': 4, 'text': 'Why are you reading this ? You hate this kind of stuff .', 'speaker': 'Monica', 'emotion': 'surprise'}, {'utterance_ID': 5, 'text': 'Yeah I know , but I figured a shot you know ?', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'Maybe one of those stories would make me cry and then you would not think I was you know , all dead inside .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'Oh that is so sweet ! Look Chandler I do not care if you can not cry , I love you .', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 8, 'text': 'Oh that makes me feel so warm in my hollow tin chest .', 'speaker': 'Chandler', 'emotion': 'sadness'}, {'utterance_ID': 9, 'text': 'Stop it !', 'speaker': 'Monica', 'emotion': 'joy'}, {'utterance_ID': 10, 'text': 'No , I mean , come on , seriously think about it , we get married , we are up at the altar and I am like this .', 'speaker': 'Chandler', 'emotion': 'sadness'}, {'utterance_ID': 11, 'text': 'I will not care , because I know you will be feeling it all in here .', 'speaker': 'Monica', 'emotion': 'neutral'}, {'utterance_ID': 12, 'text': 'Yeah ?', 'speaker': 'Chandler', 'emotion': 'surprise'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'This is what I have got going for the party so far , liquor wise . Get a lot of liquor .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Great . Great .', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 3, 'text': 'Okay , now uh , in terms of the invite list , I have got you , me , and Chandler and I am gonna invite Gunther cause , well , we have been talking about this pretty loud .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'I will be there .', 'speaker': 'Gunther', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': 'Listen , I know this is your party , but I would really like to the number of museum geeks that are gonna be there .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'Yeah . tell you what , let not invite any of the anthropologists , only the dinosaur dudes !', 'speaker': 'Ross', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'Okay ! We will need a six ... pack of Zima .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'Hey guys , what are you doing ?', 'speaker': 'Chandler', 'emotion': 'neutral'}]\n",
      "Conversation is not a dictionary or missing 'conversation' key: [{'utterance_ID': 1, 'text': 'Hey .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 2, 'text': 'Hey ! Any good mail ?', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 3, 'text': 'Yes , you got something from the Screen Actor Guild .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 4, 'text': 'Ooh , it is probably a residual check , hey can you open it for me , I am kinda ...', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 5, 'text': '\" Benefits lapsed . \"', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 6, 'text': 'Hmm that is weird . I do not remember being in a move called benefits lapsed .', 'speaker': 'Joey', 'emotion': 'neutral'}, {'utterance_ID': 7, 'text': 'Okay , it is not a check . They are saying your health insurance expired because , you did not work enough last year .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 8, 'text': 'Let me see that !', 'speaker': 'Joey', 'emotion': 'surprise'}, {'utterance_ID': 9, 'text': 'All right .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 10, 'text': 'Oh , I can not believe this !', 'speaker': 'Joey', 'emotion': 'surprise'}, {'utterance_ID': 11, 'text': 'This sucks !', 'speaker': 'Joey', 'emotion': 'anger'}, {'utterance_ID': 12, 'text': 'When I had insurance I could get hit by a bus or catch on fire , you know ?', 'speaker': 'Joey', 'emotion': 'fear'}, {'utterance_ID': 13, 'text': 'And it would not matter .', 'speaker': 'Joey', 'emotion': 'anger'}, {'utterance_ID': 14, 'text': 'Now I gotta be careful ?', 'speaker': 'Joey', 'emotion': 'anger'}, {'utterance_ID': 15, 'text': 'I am sorry man , there is never a good time to stop catching on fire .', 'speaker': 'Chandler', 'emotion': 'disgust'}, {'utterance_ID': 16, 'text': 'All right well , I guess I gotta go get a job . I am gonna go see my agent .', 'speaker': 'Joey', 'emotion': 'sadness'}, {'utterance_ID': 17, 'text': 'Okay , make sure you look both ways before you cross the street .', 'speaker': 'Chandler', 'emotion': 'neutral'}, {'utterance_ID': 18, 'text': 'look both ways before you cross the street .', 'speaker': 'Joey', 'emotion': 'neutral'}]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No texts found for tokenization",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28692\\4218558878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Tokenize + encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_input_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_input_batches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdev_input_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28692\\4218558878.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Tokenize + encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtrain_input_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_input_batches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdev_input_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28692\\3514150154.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, conv_batch)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcombined_texts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No texts found for tokenization\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No texts found for tokenization"
     ]
    }
   ],
   "source": [
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "batch_size = 16\n",
    "\n",
    "for batch in chunk(x_train, batch_size):\n",
    "    if not batch:\n",
    "        print(\"Found an empty batch in x_train\")\n",
    "    else:\n",
    "        print(\"Batch size:\", len(batch))\n",
    "\n",
    "\n",
    "tokenizer = BatchTokenizer()  # Ensure this is correctly defined\n",
    "\n",
    "train_input_batches = [b for b in chunk(x_train, batch_size)]\n",
    "# print(\"Train batch example:\", train_input_batches[0])  # Debug: Check the first batch\n",
    "\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(batch) for batch in train_input_batches]\n",
    "\n",
    "dev_input_batches = [b for b in chunk(x_dev, batch_size)]\n",
    "print(\"Dev batch example:\", dev_input_batches[0])  # Debug: Check the first batch\n",
    "\n",
    "# Tokenize + encode\n",
    "dev_input_batches = [tokenizer(batch) for batch in dev_input_batches]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fd0db-eda2-47a4-84e1-6b9811fe9513",
   "metadata": {},
   "source": [
    "## Tokenize and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a0c3c-c620-4dfd-aa35-4043d9a8d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_list, emotion_list = data_to_conversation_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6835e-677b-4d26-988e-7c13055c5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947967b-1ccc-4b4b-b007-b5aafc8924e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emotion_list = []\n",
    "for emotion in emotion_list:\n",
    "    if emotion not in unique_emotion_list:\n",
    "        unique_emotion_list.append(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e7411-bea8-4fe7-859c-86810e1f3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens to be removed\n",
    "special_tokens = {'<SOS>', '<EOS>', '<PAD>'}\n",
    "\n",
    "# Joining each sublist into strings and then joining these strings\n",
    "conversation_string = ' '.join(\n",
    "    ' '.join(\n",
    "        ' '.join(word for word in inner_list if word not in special_tokens) \n",
    "        for inner_list in sublist\n",
    "    ) \n",
    "    for sublist in conversation_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e4d06-073e-4dbe-b011-253c4b715cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe43e0-b55c-413d-a07f-3de5312b61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def get_sentence_embedding(sentence, model_name=\"bert-base-uncased\"):\n",
    "    # Load pre-trained model tokenizer\n",
    "    #tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Encode text\n",
    "    #encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Example text\n",
    "    \n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer.encode_plus(\n",
    "        sentence, \n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=512,\n",
    "        truncation=True, # Pad & truncate all sentences.\n",
    "        padding='max_length',\n",
    "        #padding=True,\n",
    "        return_attention_mask=True,   # Construct attention masks.\n",
    "        return_tensors='pt',          # Return PyTorch tensors.\n",
    "    )\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Forward pass, get hidden states\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    # Get the embeddings of the [CLS] token (first token), representing the entire sentence\n",
    "    sentence_embedding = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca717dca-e4c1-4114-9b4f-22318a59a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_embeddings = get_sentence_embedding(conversation_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21b63e-40d2-4e7a-859d-360ae392a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dd9e1-845d-469a-8d4a-8dc7b3c12786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the list to a set to get unique elements, and then back to a list\n",
    "unique_emotion_list = []\n",
    "for emotion in emotion_list:\n",
    "    if emotion not in unique_emotion_list:\n",
    "        unique_emotion_list.append(emotion)\n",
    "\n",
    "#print(unique_emotion_list)\n",
    "print(conversation_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffad4b0-ea64-4a60-8d25-d4ac7c44f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming 'conversation_embeddings' is a numpy array of your embeddings\n",
    "# And 'labels' is a list or array of your emotion labels\n",
    "\n",
    "labels = np.array(['joy', 'sadness', 'surprise', 'disgust', 'fear', 'neutral'])  # Replace with your labels\n",
    "conversation_embeddings_req = np.array(conversation_embeddings)  # This should be a 2D array\n",
    "labels_new = np.array(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(conversation_embeddings_req, labels_new, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db833a5-dd2a-4938-9438-633ea1821e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
