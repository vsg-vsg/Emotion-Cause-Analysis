{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b73597b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'V:\\\\UCB Coursework\\\\Fall23_courses\\\\Natural Language Processing\\\\Project\\\\Emotion-Cause-Analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b1762",
   "metadata": {},
   "source": [
    "### Logistic Regression using tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7985ed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.39      0.17      0.24       294\n",
      "     disgust       0.00      0.00      0.00        82\n",
      "        fear       0.00      0.00      0.00        89\n",
      "         joy       0.57      0.31      0.41       480\n",
      "     neutral       0.54      0.89      0.67      1211\n",
      "     sadness       0.38      0.12      0.18       226\n",
      "    surprise       0.62      0.46      0.53       342\n",
      "\n",
      "    accuracy                           0.54      2724\n",
      "   macro avg       0.36      0.28      0.29      2724\n",
      "weighted avg       0.49      0.54      0.48      2724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data from JSON\n",
    "f = open('.\\Data\\\\text\\Subtask_1_train.json', encoding=\"utf-8\")\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "# Extract texts and emotions\n",
    "texts = [utterance['text'] for conv in data for utterance in conv['conversation']]\n",
    "emotions = [utterance['emotion'] for conv in data for utterance in conv['conversation']]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, emotions, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742b65a",
   "metadata": {},
   "source": [
    "###  Import data and import libraries for usage by the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd127628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "import math\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('.\\Data\\\\text\\Subtask_1_train.json', encoding=\"utf-8\")\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e7907",
   "metadata": {},
   "source": [
    "## Function to make batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dcdceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(sequences: List[str], batch_size: int) -> List[List[str]]:\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "\n",
    "    batch_list=[]\n",
    "\n",
    "    last_index=len(sequences)-1\n",
    "    \n",
    "    for index in range(math.ceil(len(sequences)/batch_size)):\n",
    "        \n",
    "        if index+batch_size:\n",
    "            batch_list.append(sequences[index:index+batch_size])\n",
    "        else:\n",
    "            batch_list.append(sequences[index:last_index])\n",
    "    # DONE\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e83ff",
   "metadata": {},
   "source": [
    "## Tokenize and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74eda2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, pad_symbol: Optional[str] = \"<PAD>\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<PAD>\".\n",
    "        \"\"\"\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def __call__(self, sentences_batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes each sentence in the batch, and pads them if necessary so\n",
    "        that we have equal length sentences in the batch.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A List of equal-length token Lists.\n",
    "        \"\"\"\n",
    "        sentences_batch = self.tokenize(sentences_batch)\n",
    "        sentences_batch = self.pad(sentences_batch)\n",
    "\n",
    "        return sentences_batch\n",
    "\n",
    "    def tokenize(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): The input sentence.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The tokenized version of the sentence.\n",
    "        \"\"\"\n",
    "        ret_tokenized_sentences = []\n",
    "        # for token in self.nlp(sentences):\n",
    "        #     ret_tokenized_sentences.append(token.text)\n",
    "            \n",
    "        ret_tokenized_sentences = []\n",
    "        for sentence_iter in sentences:\n",
    "            token_list = ['<SOS>']\n",
    "            # token_list = []\n",
    "            for token in self.nlp(sentence_iter):\n",
    "                token_list.append(token.text)\n",
    "            token_list.append('<EOS>')\n",
    "            ret_tokenized_sentences.append(token_list)\n",
    "        return ret_tokenized_sentences\n",
    "    \n",
    "    def pad(self, batch: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Appends pad symbols to each tokenized sentence in the batch such that\n",
    "        every List of tokens is the same length. This means that the max length sentence\n",
    "        will not be padded.\n",
    "\n",
    "        Args:\n",
    "            batch (List[List[str]]): Batch of tokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: Batch of padded tokenized sentences. \n",
    "        \"\"\"\n",
    "        ret_batch = []\n",
    "        max_len = len(max(batch, key=len))\n",
    "\n",
    "        ret_batch = [sentence_tokens + ['<P>'] * (max_len - len(sentence_tokens)) for sentence_tokens in batch]\n",
    "\n",
    "        for sentence in batch:\n",
    "            for i in range(max_len - len(sentence)):\n",
    "                sentence.append(self.pad_symbol)\n",
    "\n",
    "        return batch\n",
    "        \n",
    "        # return ret_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e796efc",
   "metadata": {},
   "source": [
    "### Conversion of data to usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc02fc66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_to_conversation_list(data) -> (List[List[str]], List[List[str]]):\n",
    "    # Create a list to store the data\n",
    "    conversation_data = []\n",
    "    emotion_list = []\n",
    "\n",
    "    # Iterate through conversations\n",
    "    for conversation in data:\n",
    "        conversation_id = conversation[\"conversation_ID\"]\n",
    "        utterances = conversation[\"conversation\"]\n",
    "        \n",
    "        # We make batches now and use those.\n",
    "        # tokenized_data = []\n",
    "        # Note: Labels need to be batched in the same way to ensure\n",
    "        # We have train sentence and label batches lining up.\n",
    "        #for batch in make_batches(utterances['text'], batch_size):\n",
    "        #    tokenized_data.append(tokenizer(batch))\n",
    "        # tokenized_data = my_tokenizer.tokenize(utterances)\n",
    "        \n",
    "        # print(\"After tokenization:\", tokenized_data)\n",
    "        \n",
    "        emotion_cause_pairs = conversation[\"emotion-cause_pairs\"]\n",
    "\n",
    "        utterance_data = []\n",
    "        emotion_temp = []\n",
    "        # Process each utterance in the conversation\n",
    "        for utterance in utterances:\n",
    "            utterance_id = utterance[\"utterance_ID\"]\n",
    "            #print(\"Before tokenization:\", utterance[\"text\"], \"\\n After tokenization:\", tokenizer(utterance[\"text\"]))\n",
    "            text = utterance[\"speaker\"] + \": \" + utterance[\"text\"]\n",
    "            #speaker = utterance[\"speaker\"]\n",
    "            emotion = utterance[\"emotion\"]\n",
    "\n",
    "            # Append the data to the list\n",
    "            # conversation_data.append([conversation_id, utterance_id, text, emotion]) #speaker, emotion])\n",
    "            utterance_data.append(text)\n",
    "            emotion_temp.append(emotion)\n",
    "            #print(utterance_data, emotion_temp)\n",
    "            assert len(utterance_data) == len(emotion_temp)\n",
    "        conversation_data.append(utterance_data) #speaker, emotion])\n",
    "        emotion_list.append(emotion_temp)\n",
    "        assert len(conversation_data) == len(emotion_list)\n",
    "    \n",
    "    return conversation_data, emotion_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd10d47",
   "metadata": {},
   "source": [
    "## Function to retrieve sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "884c77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def get_sentence_embedding(sentence, model_name=\"bert-base-uncased\"):\n",
    "    # Load pre-trained model tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Encode text\n",
    "    encoded_input = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Forward pass, get hidden states\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "\n",
    "    # Get the embeddings of the [CLS] token (first token), representing the entire sentence\n",
    "    sentence_embedding = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03135a6b-29fc-483e-b76c-91fa5c01adb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Special tokens to be removed\\nspecial_tokens = {'<SOS>', '<EOS>', '<PAD>'}\\n\\n# Joining each sublist into strings and then joining these strings\\nconversation_string = ' '.join(\\n    ' '.join(\\n        ' '.join(word for word in inner_list if word not in special_tokens) \\n        for inner_list in sublist\\n    ) \\n    for sublist in conversation_list\\n)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Special tokens to be removed\n",
    "special_tokens = {'<SOS>', '<EOS>', '<PAD>'}\n",
    "\n",
    "# Joining each sublist into strings and then joining these strings\n",
    "conversation_string = ' '.join(\n",
    "    ' '.join(\n",
    "        ' '.join(word for word in inner_list if word not in special_tokens) \n",
    "        for inner_list in sublist\n",
    "    ) \n",
    "    for sublist in conversation_list\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef6996",
   "metadata": {},
   "source": [
    "## Logistic Regression using embeddings from Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f9416",
   "metadata": {},
   "source": [
    "Do not run as it runs forever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e28794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Your existing code for Tokenizer, data processing, and get_sentence_embedding functions...\n",
    "\n",
    "# Preprocess data and obtain BERT embeddings for each utterance\n",
    "conversation_data, emotion_list = data_to_conversation_list(data)\n",
    "\n",
    "# Generate BERT embeddings for each utterance\n",
    "bert_embeddings = []\n",
    "for conv in conversation_data:\n",
    "    conv_embeddings = []\n",
    "    for utterance in conv:\n",
    "        # Get BERT embedding for each utterance\n",
    "        embedding = get_sentence_embedding(utterance)\n",
    "        conv_embeddings.append(embedding)\n",
    "    bert_embeddings.append(conv_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ed560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Flatten the list of embeddings for each utterance in a conversation\n",
    "flat_embeddings = [emb for conv in bert_embeddings for emb in conv]\n",
    "\n",
    "# Flatten the list of emotions for each utterance in a conversation\n",
    "flat_emotions = [emo for sublist in emotion_list for emo in sublist]\n",
    "\n",
    "# Convert embeddings and emotions to numpy arrays\n",
    "X = torch.cat(flat_embeddings).numpy()  # Convert BERT embeddings to numpy array\n",
    "y = np.array(flat_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a33231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33026d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predict on test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ab3da",
   "metadata": {},
   "source": [
    "### Logistic Regression using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba52a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess data and obtain BERT embeddings for each utterance\n",
    "conversation_data, emotion_list = data_to_conversation_list(data)\n",
    "\n",
    "# Train Word2Vec model\n",
    "#word2vec_model = Word2Vec(sentences=conversation_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "# Tokenize sentences into words\n",
    "tokenized_data = [word_tokenize(\" \".join(conv)) for conv in conversation_data]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "# Generate embeddings for each utterance\n",
    "word2vec_embeddings = []\n",
    "for conv in conversation_data:\n",
    "\n",
    "    conv_embeddings = []\n",
    "    for utterance in conv:\n",
    "\n",
    "        tokens = word_tokenize(utterance)\n",
    "\n",
    "\n",
    "        embedding_intermediate = [word2vec_model.wv[str(word)] for word in tokens if word in word2vec_model.wv]\n",
    "\n",
    "        embedding = np.mean(pd.Series(embedding_intermediate, dtype=object).fillna(0).tolist(), axis=0)\n",
    "\n",
    "        conv_embeddings.append(embedding)\n",
    "    word2vec_embeddings.append(conv_embeddings)\n",
    "\n",
    "# Flatten the list of embeddings for each utterance in a conversation\n",
    "flat_embeddings = [emb for conv in word2vec_embeddings for emb in conv]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a8a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.44      0.23      0.30       294\n",
      "     disgust       0.00      0.00      0.00        82\n",
      "        fear       0.75      0.07      0.12        89\n",
      "         joy       0.46      0.23      0.30       480\n",
      "     neutral       0.55      0.92      0.69      1211\n",
      "     sadness       0.32      0.05      0.09       226\n",
      "    surprise       0.57      0.45      0.50       342\n",
      "\n",
      "    accuracy                           0.54      2724\n",
      "   macro avg       0.44      0.28      0.29      2724\n",
      "weighted avg       0.50      0.54      0.47      2724\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vivek\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of emotions for each utterance in a conversation\n",
    "flat_emotions = [emo for sublist in emotion_list for emo in sublist]\n",
    "\n",
    "# Convert embeddings and emotions to numpy arrays\n",
    "X = np.array(flat_embeddings)\n",
    "y = np.array(flat_emotions)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# print(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Initialize and train logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878b793",
   "metadata": {},
   "source": [
    "## Glove embeddings (Uncomment if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! tar -xf glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022771c",
   "metadata": {},
   "source": [
    "## Doc2Vec (Use if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b172b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03154751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for utterance_list in conversation_list:\n",
    "    tagged_data = [TaggedDocument(_d, tags=[str(i)]) for i, _d in enumerate(utterance_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae90072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9056969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"I love chatbots\".lower())\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('1')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
    "print(model.docvecs['1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66aabb",
   "metadata": {},
   "source": [
    "## Encode documents of coversations from pre trained doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def encode_documents(documents: List[List[List[str]]], model: Doc2Vec) -> torch.FloatTensor:\n",
    "    \"\"\"Encode the list of documents using a pre-trained Doc2Vec model.\n",
    "\n",
    "    Args:\n",
    "        documents (List[str]): List of documents to encode.\n",
    "        model (Doc2Vec): Pre-trained Doc2Vec model.\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of encoded documents.\n",
    "    \"\"\"\n",
    "    encoded_docs = []\n",
    "    for conversation in documents:\n",
    "        conversation_embeddings = [model.infer_vector(tokens, epochs=20) for tokens in conversation]\n",
    "        encoded_docs.extend(conversation_embeddings)\n",
    "    return torch.FloatTensor(encoded_docs)\n",
    "\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[List[str]]) -> List[torch.FloatTensor]:\n",
    "    \"\"\"Convert string labels into numeric tensors.\n",
    "\n",
    "    Args:\n",
    "        labels (List[List[str]]): Nested list of string labels.\n",
    "\n",
    "    Returns:\n",
    "        List[torch.FloatTensor]: List of tensors containing the labels.\n",
    "    \"\"\"\n",
    "    label_dict = {'neutral': 0, 'surprise': 1, 'anger': 2, 'sadness': 3, 'happy': 4}  # Define your label mapping\n",
    "    encoded_labels = []\n",
    "    for nested_list in labels:\n",
    "        # Map string labels to numeric values\n",
    "        numeric_labels = [label_dict.get(label, -1) for label in nested_list]  # Use -1 as default value\n",
    "        # Remove -1 values (labels not found in the dictionary)\n",
    "        numeric_labels = [label for label in numeric_labels if label != -1]\n",
    "        encoded_labels.append(torch.FloatTensor(numeric_labels))\n",
    "    return encoded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3edd3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode conversations\n",
    "encoded_conversations = encode_documents(conversation_list, model)\n",
    "\n",
    "# Emotion labels encoding remains the same as before\n",
    "encoded_emotion_labels = encode_labels(emotion_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4d8b0",
   "metadata": {},
   "source": [
    "## Get OOVs from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary list\n",
    "vocab_list = list(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def get_oovs(vocab_list: List[str], model:Doc2Vec) -> List[str]:\n",
    "    \"\"\"Find the words in vocab_list that are out-of-vocabulary (OOV) in the Doc2Vec model.\n",
    "    \n",
    "    Args:\n",
    "    vocab_list (List[str]): List of words in your vocabulary.\n",
    "    model: Pre-trained Doc2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: List of out-of-vocabulary words in vocab_list.\n",
    "    \"\"\"\n",
    "    oov_words = [word for word in vocab_list if word not in model.wv]\n",
    "    return oov_words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
